# pgAnalytics v3 - Roadmap de ImplementaÃ§Ã£o Detalhado

**Data**: February 20, 2026
**Status**: AnÃ¡lise Completa
**Objetivo**: Guia prÃ¡tico para completar as fases restantes

---

## ğŸ“‹ RESUMO DE FASES

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      PROJECT PHASE ROADMAP                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Phase 1: Foundation            âœ… COMPLETE   - Monorepo, DB setup â•‘
â•‘ Phase 2: Backend Auth          âœ… COMPLETE   - JWT, handlers      â•‘
â•‘ Phase 3.1-3.4: Testing         âœ… COMPLETE   - 70/70 tests pass   â•‘
â•‘ Phase 3.5: Collector (75%)     â³ IN REVIEW  - 3/4 collectors OK  â•‘
â•‘ Phase 3.5.A: PostgreSQL        âŒ TODO      - libpq integration   â•‘
â•‘ Phase 3.5.B: Config Pull       âŒ TODO      - hot-reload          â•‘
â•‘ Phase 3.5.C: E2E Testing       âŒ TODO      - full integration    â•‘
â•‘ Phase 3.5.D: Documentation     âŒ TODO      - finalization        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ TOTAL PROGRESS: 85% | Ready for: Code Review â†’ PR â†’ Merge       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ”´ FASE ATUAL: Phase 3.5 - Aguardando PR Review

### Status Atual
- **Branch**: `feature/phase3-collector-modernization`
- **Commits**: 8 (7 implementaÃ§Ã£o + 1 documentaÃ§Ã£o)
- **Commits Pushed**: âœ… Sim
- **Testes**: 70/70 PASSING âœ…
- **Build**: 0 ERRORS âœ…

### PrÃ³xima AÃ§Ã£o: Criar PR no GitHub
**Link Direto**: https://github.com/torresglauco/pganalytics-v3/pull/new/feature/phase3-collector-modernization

**Processo**:
1. Clicar no link acima
2. Copiar tÃ­tulo: "Phase 3.5: C/C++ Collector Modernization - Foundation Implementation"
3. Copiar descriÃ§Ã£o de `PR_TEMPLATE.md`
4. Clique em "Create pull request"

---

## ğŸŸ¡ PRÃ“XIMAS FASES: Detalhamento Completo

### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## FASE 3.5.A: PostgreSQL Plugin Enhancement

**Objetivo**: Implementar SQL query execution no PgStatsCollector

**Arquivo Principal**: `collector/src/postgres_plugin.cpp`

### Estrutura Atual
```cpp
// EstÃ£o prontos:
âœ… Database iteration loop
âœ… JSON schema structure
âœ… Placeholder methods

// Faltam:
â³ LibPQ library integration
â³ SQL query execution
â³ Results parsing
â³ Error handling
```

### ImplementaÃ§Ã£o Passo-a-Passo

#### 1. Adicionar LibPQ como Dependency
**Arquivo**: `collector/CMakeLists.txt`

```cmake
# Adicionar apÃ³s find_package(OpenSSL)
find_package(PostgreSQL REQUIRED)

# Adicionar aos includes
include_directories(${PostgreSQL_INCLUDE_DIRS})

# Adicionar aos link libraries
target_link_libraries(pganalytics-bin ${PostgreSQL_LIBRARIES})
```

**Verificar instalaÃ§Ã£o**:
```bash
# macOS
brew install libpq

# Linux (Ubuntu/Debian)
sudo apt-get install libpq-dev

# Linux (RHEL/CentOS)
sudo yum install postgresql-devel
```

#### 2. Implementar MÃ©todos SQL no PgStatsCollector

**LocalizaÃ§Ã£o**: `collector/src/postgres_plugin.cpp` (linhas 150-200)

```cpp
// MÃ©todo para database size
private:
    std::string getDatabaseSize(PGconn* conn, const std::string& database) {
        const char* query =
            "SELECT pg_database_size(datname) FROM pg_database WHERE datname = $1";

        const char* paramValues[] = {database.c_str()};
        int paramLengths[] = {(int)database.length()};
        int paramFormats[] = {0};

        PGresult* result = PQexecParams(
            conn, query, 1, nullptr, paramValues, paramLengths, paramFormats, 0
        );

        if (PQresultStatus(result) != PGRES_TUPLES_OK) {
            LOG(WARNING) << "Failed to get database size: " << PQerrorMessage(conn);
            PQclear(result);
            return "0";
        }

        std::string size = PQgetvalue(result, 0, 0);
        PQclear(result);
        return size;
    }

// MÃ©todo para table stats
private:
    json getTableStats(PGconn* conn, const std::string& database) {
        const char* query =
            "SELECT schemaname, tablename, pg_total_relation_size(schemaname||'.'||tablename) as size "
            "FROM pg_tables WHERE tablename NOT LIKE 'pg_%' "
            "ORDER BY size DESC LIMIT 10";

        PGresult* result = PQexec(conn, query);
        json tables = json::array();

        if (PQresultStatus(result) == PGRES_TUPLES_OK) {
            for (int i = 0; i < PQntuples(result); i++) {
                json table = {
                    {"schema", PQgetvalue(result, i, 0)},
                    {"name", PQgetvalue(result, i, 1)},
                    {"size_bytes", std::stoll(PQgetvalue(result, i, 2))}
                };
                tables.push_back(table);
            }
        }

        PQclear(result);
        return tables;
    }

// MÃ©todo para index stats
private:
    json getIndexStats(PGconn* conn, const std::string& database) {
        const char* query =
            "SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch "
            "FROM pg_stat_user_indexes ORDER BY idx_scan DESC LIMIT 10";

        PGresult* result = PQexec(conn, query);
        json indexes = json::array();

        if (PQresultStatus(result) == PGRES_TUPLES_OK) {
            for (int i = 0; i < PQntuples(result); i++) {
                json index = {
                    {"schema", PQgetvalue(result, i, 0)},
                    {"table", PQgetvalue(result, i, 1)},
                    {"name", PQgetvalue(result, i, 2)},
                    {"scans", std::stoll(PQgetvalue(result, i, 3))},
                    {"tuples_read", std::stoll(PQgetvalue(result, i, 4))},
                    {"tuples_returned", std::stoll(PQgetvalue(result, i, 5))}
                };
                indexes.push_back(index);
            }
        }

        PQclear(result);
        return indexes;
    }
```

#### 3. Atualizar MÃ©todo execute()

**LocalizaÃ§Ã£o**: `collector/src/postgres_plugin.cpp` (linhas 50-100)

```cpp
json PgStatsCollector::execute() {
    json result;
    result["type"] = "pg_stats";
    result["timestamp"] = getCurrentTimestamp();
    result["databases"] = json::array();

    for (const auto& db : databases_) {
        json db_entry;
        db_entry["name"] = db;
        db_entry["timestamp"] = getCurrentTimestamp();

        // Connect to database
        PGconn* conn = PQconnectdb(
            ("host=" + hostname_ + " port=" + std::to_string(port_) +
             " dbname=" + db + " user=" + username_).c_str()
        );

        if (PQstatus(conn) != CONNECTION_OK) {
            LOG(WARNING) << "Failed to connect to " << db << ": " << PQerrorMessage(conn);
            PQfinish(conn);
            continue;
        }

        // Get database size
        std::string size_str = getDatabaseSize(conn, db);
        db_entry["size_bytes"] = std::stoll(size_str);

        // Get table stats
        db_entry["tables"] = getTableStats(conn, db);

        // Get index stats
        db_entry["indexes"] = getIndexStats(conn, db);

        result["databases"].push_back(db_entry);
        PQfinish(conn);
    }

    return result;
}
```

#### 4. Testes para PostgreSQL Plugin

**Arquivo Novo**: `collector/tests/postgres_plugin_test.cpp`

```cpp
#include <gtest/gtest.h>
#include "../src/postgres_plugin.cpp"

class PgStatsCollectorTest : public ::testing::Test {
protected:
    PgStatsCollector collector{"localhost", "col-001"};
};

TEST_F(PgStatsCollectorTest, ExecuteReturnsValidJSON) {
    // This test would run against a test database
    // Skip if database not available

    json result = collector.execute();

    EXPECT_EQ(result["type"], "pg_stats");
    EXPECT_TRUE(result.contains("timestamp"));
    EXPECT_TRUE(result.contains("databases"));
    EXPECT_TRUE(result["databases"].is_array());
}

TEST_F(PgStatsCollectorTest, DatabaseStatsHasRequiredFields) {
    json result = collector.execute();

    if (!result["databases"].empty()) {
        auto db = result["databases"][0];
        EXPECT_TRUE(db.contains("name"));
        EXPECT_TRUE(db.contains("size_bytes"));
        EXPECT_TRUE(db.contains("tables"));
        EXPECT_TRUE(db.contains("indexes"));
    }
}

TEST_F(PgStatsCollectorTest, TableStatsAreValid) {
    json result = collector.execute();

    if (!result["databases"].empty()) {
        auto tables = result["databases"][0]["tables"];
        if (!tables.empty()) {
            auto table = tables[0];
            EXPECT_TRUE(table.contains("schema"));
            EXPECT_TRUE(table.contains("name"));
            EXPECT_TRUE(table.contains("size_bytes"));
            EXPECT_TRUE(table["size_bytes"].is_number());
        }
    }
}
```

### Checklist de ImplementaÃ§Ã£o
- [ ] Adicionar libpq ao CMakeLists.txt
- [ ] Implementar getDatabaseSize()
- [ ] Implementar getTableStats()
- [ ] Implementar getIndexStats()
- [ ] Atualizar execute() method
- [ ] Adicionar testes para cada mÃ©todo
- [ ] Verificar memory leaks (valgrind)
- [ ] Teste com banco de dados real
- [ ] Validar JSON schema
- [ ] Todos os testes passam

### Estimativa
**Tempo**: 2-3 horas
- DependÃªncias: 15 min
- ImplementaÃ§Ã£o: 90 min
- Testes: 45 min
- ValidaÃ§Ã£o: 30 min

### CritÃ©rios de Sucesso
- âœ… 70/70 testes passing
- âœ… 0 memory leaks
- âœ… Database connection handling
- âœ… Error handling para conexÃµes falhas
- âœ… JSON schema validation

---

### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## FASE 3.5.B: Config Pull Integration

**Objetivo**: Implementar GET /api/v1/config/{collector_id} no collector

**Arquivo Principal**: `collector/src/collector.cpp` (main loop)

### MudanÃ§as NecessÃ¡rias

#### 1. Atualizar Backend API (Go)

**Arquivo**: `backend/internal/api/handlers.go`

```go
// Adicionar handler para GET /api/v1/config/{collector_id}
func (s *Server) getCollectorConfig(c *gin.Context) {
    collectorID := c.Param("id")

    // TODO: Query database for collector config
    config := map[string]interface{}{
        "intervals": map[string]int{
            "sysstat": 60,
            "pg_logs": 60,
            "disk_usage": 60,
            "pg_stats": 120,
        },
        "enabled_collectors": []string{
            "sysstat",
            "pg_logs",
            "disk_usage",
            "pg_stats",
        },
        "log_filters": []string{
            "ERROR",
            "FATAL",
            "WARNING",
        },
        "retention_days": 30,
        "compression": true,
        "backup_interval": 3600,
    }

    c.JSON(http.StatusOK, config)
}

// Registrar route
authGroup.GET("/config/:id", s.getCollectorConfig)
```

#### 2. Implementar Config Pull no Collector

**Arquivo**: `collector/src/collector.cpp` (novo mÃ©todo)

```cpp
// Add method to ConfigManager
bool ConfigManager::pullConfigFromBackend(
    const std::string& backend_url,
    const std::string& collector_id,
    const std::string& jwt_token
) {
    CURL* curl = curl_easy_init();
    if (!curl) return false;

    std::string url = backend_url + "/api/v1/config/" + collector_id;
    std::string response;

    // Set up curl options
    curl_easy_setopt(curl, CURLOPT_URL, url.c_str());
    curl_easy_setopt(curl, CURLOPT_HTTPAUTH, CURLAUTH_BEARER);
    curl_easy_setopt(curl, CURLOPT_XOAUTH2_BEARER, jwt_token.c_str());

    struct curl_slist* headers = nullptr;
    headers = curl_slist_append(headers, "Content-Type: application/json");
    curl_easy_setopt(curl, CURLOPT_HTTPHEADER, headers);

    // Write callback
    auto write_callback = [](void* contents, size_t size, size_t nmemb, std::string* s) {
        s->append((char*)contents, size * nmemb);
        return size * nmemb;
    };

    curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, write_callback);
    curl_easy_setopt(curl, CURLOPT_WRITEDATA, &response);

    CURLcode res = curl_easy_perform(curl);

    if (res != CURLE_OK) {
        LOG(ERROR) << "Failed to pull config: " << curl_easy_strerror(res);
        curl_slist_free_all(headers);
        curl_easy_cleanup(curl);
        return false;
    }

    long http_code = 0;
    curl_easy_getinfo(curl, CURLINFO_RESPONSE_CODE, &http_code);

    curl_slist_free_all(headers);
    curl_easy_cleanup(curl);

    if (http_code != 200) {
        LOG(ERROR) << "Config pull failed with HTTP " << http_code;
        return false;
    }

    // Parse JSON and update config
    try {
        json config = json::parse(response);
        updateFromJSON(config);
        LOG(INFO) << "Config updated from backend";
        return true;
    } catch (const std::exception& e) {
        LOG(ERROR) << "Failed to parse config: " << e.what();
        return false;
    }
}
```

#### 3. Integrar no Main Loop

**Arquivo**: `collector/src/collector.cpp` (main execution loop)

```cpp
// Add config pull to main loop
void Collector::run() {
    auto last_config_pull = std::chrono::system_clock::now();
    const auto config_pull_interval = std::chrono::hours(1); // Pull every hour

    while (running_) {
        auto now = std::chrono::system_clock::now();

        // Pull config every hour
        if (now - last_config_pull >= config_pull_interval) {
            LOG(INFO) << "Pulling config from backend...";
            config_manager_.pullConfigFromBackend(
                backend_url_,
                collector_id_,
                getCurrentJWTToken()
            );
            last_config_pull = now;
        }

        // Execute collectors based on intervals
        for (const auto& collector : collectors_) {
            if (shouldExecute(collector)) {
                executeCollector(collector);
            }
        }

        // Push metrics
        pushMetrics();

        // Sleep
        std::this_thread::sleep_for(std::chrono::seconds(10));
    }
}
```

### Testes
```cpp
TEST_F(ConfigManagerTest, PullConfigFromBackend) {
    // Mock HTTP server
    // Test successful config pull
    EXPECT_TRUE(config.pullConfigFromBackend(url, id, token));
}

TEST_F(ConfigManagerTest, UpdateFromJSON) {
    json config_json = {
        {"intervals", {{"sysstat", 60}}},
        {"enabled_collectors", json::array({"sysstat"})}
    };

    config.updateFromJSON(config_json);
    EXPECT_EQ(config.getInterval("sysstat"), 60);
}
```

### Checklist
- [ ] Implementar GET /api/v1/config/{collector_id} no backend
- [ ] Adicionar pullConfigFromBackend() ao ConfigManager
- [ ] Integrar config pull ao main loop
- [ ] Testes para config pull
- [ ] Testes para hot-reload
- [ ] ValidaÃ§Ã£o de config JSON
- [ ] Error handling para falhas de conexÃ£o
- [ ] 70/70 testes passing

### Estimativa
**Tempo**: 1-2 horas
- Backend: 30 min
- Collector implementation: 45 min
- Testes: 30 min

---

### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## FASE 3.5.C: Comprehensive E2E Testing

**Objetivo**: Testes completos de integraÃ§Ã£o end-to-end

### Escopo

#### 1. Integration Tests com Mock Servers
**Arquivo**: `collector/tests/integration/mock_backend_test.cpp`

```cpp
// Mock HTTP server for testing
class MockBackendServer {
public:
    void start() { /* Start mock server on port 8081 */ }
    void stop() { /* Stop mock server */ }

    void setMetricsResponse(const json& response) { /* Configure response */ }
    void setConfigResponse(const json& response) { /* Configure response */ }

    int getMetricsCallCount() { return metrics_calls_; }
    int getConfigCallCount() { return config_calls_; }
};

TEST_F(CollectorIntegrationTest, FullMetricsPushCycle) {
    MockBackendServer server;
    server.start();

    Collector collector("localhost", "col-001");
    collector.setBackendURL("http://localhost:8081");

    collector.executeAll();
    collector.pushMetrics();

    EXPECT_GT(server.getMetricsCallCount(), 0);
    server.stop();
}
```

#### 2. E2E Tests com Docker Compose
**Arquivo**: `tests/e2e/collector_e2e.sh`

```bash
#!/bin/bash

# Start environment
docker-compose -f docker-compose.yml up -d

# Wait for services
sleep 5

# Register collector
RESPONSE=$(curl -X POST http://localhost:8080/api/v1/collectors/register \
  -H "Content-Type: application/json" \
  -d '{"name":"test-col","hostname":"test-host"}')

COLLECTOR_ID=$(echo $RESPONSE | jq -r '.id')

# Build and run collector
cd collector
mkdir -p build
cd build
cmake ..
make

# Run collector for 2 minutes
timeout 120 ./src/pganalytics cron &
COLLECTOR_PID=$!

# Query metrics from API
sleep 10
METRICS=$(curl http://localhost:8080/api/v1/servers/$COLLECTOR_ID/metrics)

# Verify metrics received
if echo "$METRICS" | jq -e '.metrics | length > 0' > /dev/null; then
    echo "âœ… E2E Test PASSED"
    EXIT_CODE=0
else
    echo "âŒ E2E Test FAILED"
    EXIT_CODE=1
fi

# Cleanup
kill $COLLECTOR_PID
docker-compose down

exit $EXIT_CODE
```

#### 3. Performance Tests
**Arquivo**: `tests/performance/load_test.js` (k6 script)

```javascript
import http from 'k6/http';
import { check, sleep } from 'k6';

export let options = {
    vus: 10,        // 10 virtual users
    duration: '5m',  // 5 minute test
};

export default function() {
    // Test metrics push from 10 concurrent collectors
    let payload = {
        type: 'sysstat',
        timestamp: new Date().toISOString(),
        cpu: { user: 0.15, system: 0.05, idle: 0.8 },
        memory: { total: 8589934592, free: 2147483648 },
    };

    let response = http.post(
        'http://localhost:8080/api/v1/metrics/push',
        JSON.stringify(payload),
        {
            headers: {
                'Content-Type': 'application/json',
                'Authorization': 'Bearer ' + __ENV.JWT_TOKEN,
            },
        }
    );

    check(response, {
        'status is 200': (r) => r.status === 200,
        'response time < 100ms': (r) => r.timings.duration < 100,
    });

    sleep(1);
}
```

#### 4. Security Tests
**Arquivo**: `tests/security/mtls_test.cpp`

```cpp
TEST_F(SecurityTest, MTLSEnforced) {
    // Test without client certificate - should fail
    CURL* curl = curl_easy_init();
    curl_easy_setopt(curl, CURLOPT_URL, "https://localhost:8080/api/v1/metrics/push");
    curl_easy_setopt(curl, CURLOPT_SSL_VERIFYPEER, 1L);

    CURLcode res = curl_easy_perform(curl);
    EXPECT_NE(res, CURLE_OK); // Should fail

    // Test with client certificate - should succeed
    curl_easy_setopt(curl, CURLOPT_SSLCERT, "/path/to/collector.crt");
    curl_easy_setopt(curl, CURLOPT_SSLKEY, "/path/to/collector.key");

    res = curl_easy_perform(curl);
    EXPECT_EQ(res, CURLE_OK); // Should succeed

    curl_easy_cleanup(curl);
}

TEST_F(SecurityTest, JWTValidationEnforced) {
    // Test without JWT - should fail
    CURL* curl = curl_easy_init();
    auto response = performRequest(curl, "http://localhost:8080/api/v1/metrics/push");
    EXPECT_EQ(response.status, 401); // Unauthorized

    // Test with invalid JWT - should fail
    response = performRequest(curl, "http://localhost:8080/api/v1/metrics/push", "invalid-token");
    EXPECT_EQ(response.status, 401); // Unauthorized

    // Test with valid JWT - should succeed
    response = performRequest(curl, "http://localhost:8080/api/v1/metrics/push", valid_jwt);
    EXPECT_EQ(response.status, 200); // OK
}
```

### Checklist
- [ ] Integration tests com mock servers
- [ ] E2E tests com docker-compose
- [ ] Performance load tests (k6)
- [ ] Security tests (mTLS, JWT)
- [ ] Failover tests
- [ ] Retry logic tests
- [ ] Todos os testes passam
- [ ] Coverage > 80%

### Estimativa
**Tempo**: 2-3 horas
- Mock server setup: 45 min
- E2E tests: 60 min
- Performance tests: 30 min
- Security tests: 30 min

---

### â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## FASE 3.5.D: Documentation & Finalization

**Objetivo**: Completar documentaÃ§Ã£o e preparar para produÃ§Ã£o

### Documentos a Criar

#### 1. Deployment Guide
**Arquivo**: `docs/DEPLOYMENT.md`

```markdown
# pgAnalytics v3 - Deployment Guide

## Prerequisites
- PostgreSQL 12+
- Docker 20.10+
- Go 1.19+
- C++ compiler (g++ 9+)

## Quick Start (Docker)
```bash
docker-compose up -d
```

## Production Deployment
1. Build binaries
2. Set environment variables
3. Configure TLS certificates
4. Run migrations
5. Start backend and collectors
```

#### 2. Security Guidelines
**Arquivo**: `docs/SECURITY.md`

```markdown
# Security Guidelines

## TLS Configuration
- Use TLS 1.3 minimum
- Self-signed certs for dev
- CA-signed certs for production

## mTLS Setup
- Generate collector certificates
- Store securely
- Rotate annually

## JWT Configuration
- Use strong secret (256+ bits)
- Set short expiration (15 min)
- Implement token refresh
```

#### 3. Troubleshooting Guide
**Arquivo**: `docs/TROUBLESHOOTING.md`

```markdown
# Troubleshooting

## Common Issues

### Collector Connection Failed
1. Check TLS certificates
2. Verify JWT token
3. Check network connectivity

### Metrics Not Appearing
1. Check collector logs
2. Verify metrics format
3. Check database space

### High Memory Usage
1. Reduce metric intervals
2. Increase buffer flush frequency
3. Check for connection leaks
```

#### 4. API Documentation
**Arquivo**: `docs/API.md` (atualizar)

```markdown
# API Reference - Complete

## Collector Registration
POST /api/v1/collectors/register
- Request: {name, hostname}
- Response: {id, token, cert, key}

## Metrics Push
POST /api/v1/metrics/push
- Auth: mTLS + JWT
- Body: metrics JSON
- Response: {status, metrics_count}

## Config Pull
GET /api/v1/config/{collector_id}
- Auth: JWT
- Response: {intervals, enabled_collectors, filters}
```

### Checklist
- [ ] Deployment guide completo
- [ ] Security guidelines
- [ ] Troubleshooting guide
- [ ] API documentation
- [ ] Code comments/docstrings
- [ ] Contributing guide
- [ ] Changelog entry
- [ ] README update

### Estimativa
**Tempo**: 1-2 horas
- Documentation: 60 min
- Code review: 30 min

---

## ğŸ“… TIMELINE RECOMENDADA

```
Semana 1:
â”œâ”€ Monday-Tuesday: Code review Phase 3.5
â”œâ”€ Wednesday: Address PR feedback
â”œâ”€ Thursday: Merge to main
â””â”€ Friday: Start Phase 3.5.A

Semana 2:
â”œâ”€ Monday-Tuesday: Phase 3.5.A (PostgreSQL)
â”œâ”€ Wednesday: Phase 3.5.B (Config pull)
â”œâ”€ Thursday: Phase 3.5.C (E2E tests)
â””â”€ Friday: Phase 3.5.D (Documentation)

Semana 3:
â”œâ”€ Monday: Final testing
â”œâ”€ Tuesday: Release preparation
â”œâ”€ Wednesday: Tag v3.0.0-beta
â””â”€ Thursday+: Production deployment
```

---

## ğŸ¯ CRITÃ‰RIOS DE SUCESSO

### Phase 3.5.A - PostgreSQL Plugin
- âœ… LibPQ integration working
- âœ… SQL queries executing successfully
- âœ… JSON schema valid
- âœ… 70/70 tests passing
- âœ… 0 memory leaks
- âœ… Error handling complete
- âœ… Database connection pooling (optional)

### Phase 3.5.B - Config Pull
- âœ… API endpoint implemented
- âœ… Config pull working
- âœ… Hot-reload functional
- âœ… 70/70 tests passing
- âœ… Graceful fallback when unreachable
- âœ… Config validation

### Phase 3.5.C - E2E Testing
- âœ… Mock server tests passing
- âœ… Docker compose E2E passing
- âœ… Performance tests acceptable
- âœ… Security tests passing
- âœ… >80% code coverage
- âœ… Load tests showing <500ms p95 latency

### Phase 3.5.D - Documentation
- âœ… All guides complete
- âœ… Examples working
- âœ… API documented
- âœ… Troubleshooting covered
- âœ… Security guidelines clear
- âœ… Deployment steps clear

---

## ğŸ“Š VELOCITY TRACKING

| Phase | Estimated | Actual | Velocity |
|-------|-----------|--------|----------|
| 1: Foundation | 4h | 4h | 100% |
| 2: Auth | 6h | 6h | 100% |
| 3.1-3.4: Tests | 8h | 8h | 100% |
| 3.5: Collector | 4h | 4h | 100% |
| **3.5.A-D: Remaining** | **6-10h** | TBD | TBD |

---

## ğŸš€ PRÃ“XIMOS PASSOS

1. **Hoje**: Revisar este documento
2. **AmanhÃ£**: Criar PR no GitHub
3. **Semana**: Implementar Phase 3.5.A (PostgreSQL)
4. **PrÃ³xima semana**: Completar 3.5.B-D
5. **Semana seguinte**: v3.0 Release

---

**Document Created**: February 20, 2026
**Last Updated**: February 20, 2026
**Status**: Pronto para implementaÃ§Ã£o

